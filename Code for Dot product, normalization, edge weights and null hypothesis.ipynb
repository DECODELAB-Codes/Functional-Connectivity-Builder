{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca6d82-3aa5-4baa-97dd-c1d89b62bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a multiline string of spike timestamps into a dictionary of neurons and their timestamps.\n",
    "# Each neuron section starts with \"neuron X:\" followed by its spike timestamps on new lines.\n",
    "\n",
    "def convert_timestamps_to_dict(text_block, neuron_prefix=\"neuron\"):\n",
    "    \"\"\"\n",
    "    Converts a multiline text block of spike timestamps into a dictionary keyed by neuron names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_block : str\n",
    "        Multiline string where each neuron's block starts with a line like 'neuron X:' \n",
    "        followed by spike timestamps on the following lines.\n",
    "\n",
    "    neuron_prefix : str, optional\n",
    "        Prefix string used to detect neuron header lines (default = 'neuron').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    neuron_data : dict\n",
    "        Dictionary mapping each neuron name to a list of float timestamps. For example:\n",
    "        neuron 1: [1654170515.71814, 1654170516.03764, 1654170516.07374]\n",
    "        neuron 2: [1654170520.123456, 1654170520.523456]\n",
    "    \"\"\"\n",
    "    lines = text_block.strip().splitlines()\n",
    "    neuron_data = {}\n",
    "    current_neuron = None\n",
    "    current_timestamps = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip blank lines\n",
    "        if line.startswith(neuron_prefix):\n",
    "            if current_neuron and current_timestamps:\n",
    "                neuron_data[current_neuron] = current_timestamps\n",
    "            current_neuron = line.rstrip(':')\n",
    "            current_timestamps = []\n",
    "        else:\n",
    "            try:\n",
    "                current_timestamps.append(float(line))\n",
    "            except ValueError:\n",
    "                raise ValueError(f\"Invalid timestamp: {line}\")\n",
    "\n",
    "    if current_neuron and current_timestamps:\n",
    "        neuron_data[current_neuron] = current_timestamps\n",
    "\n",
    "    return neuron_data\n",
    "\n",
    "# === Example usage ===\n",
    "\n",
    "example_text = \"\"\"\n",
    "neuron 1:\n",
    "1654170515.718140\n",
    "1654170516.037640\n",
    "1654170516.073740\n",
    "neuron 2:\n",
    "1654170520.123456\n",
    "1654170520.523456\n",
    "\"\"\"\n",
    "\n",
    "neuron_data = convert_timestamps_to_dict(example_text)\n",
    "print(\"Parsed neuron data:\")\n",
    "for neuron, timestamps in neuron_data.items():\n",
    "    print(f\"{neuron}: {timestamps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad564a69-83a8-41b0-b7b7-791989279f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes neuron spike timestamps into normalized firing rates,\n",
    "# calculates dot products over sliding time windows,\n",
    "# and saves both firing rates and dot product matrices to Excel.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def find_earliest_timestamp(neuron_data):\n",
    "    \"\"\"\n",
    "    Finds the earliest timestamp across all neurons.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neuron_data : dict\n",
    "        Dictionary mapping neuron names to lists of timestamps (float).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Earliest timestamp in the dataset.\n",
    "    \"\"\"\n",
    "    earliest = float('inf')\n",
    "    for timestamps in neuron_data.values():\n",
    "        if timestamps:\n",
    "            earliest = min(earliest, timestamps[0])\n",
    "    return earliest\n",
    "\n",
    "def adjust_timestamps_and_calculate_rates(neuron_data, bin_size, common_start, total_duration=600):\n",
    "    \"\"\"\n",
    "    Adjusts spike timestamps to a common time frame and bins firing rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neuron_data : dict\n",
    "        Dictionary mapping each neuron to list of spike timestamps.\n",
    "    bin_size : float\n",
    "        Size of each time bin in seconds (e.g., 10).\n",
    "    common_start : float\n",
    "        Global start time for normalization (e.g., min timestamp across all neurons).\n",
    "    total_duration : float\n",
    "        Total duration in seconds (default 600s).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adjusted_data : dict\n",
    "        Dictionary of adjusted spike timestamps per neuron.\n",
    "    firing_rates : dict\n",
    "        Dictionary of firing rates per bin per neuron.\n",
    "    \"\"\"\n",
    "    adjusted_data = {}\n",
    "    firing_rates = {}\n",
    "    end_time = common_start + total_duration\n",
    "    bins = np.arange(0, total_duration + bin_size, bin_size)\n",
    "\n",
    "    for neuron, timestamps in neuron_data.items():\n",
    "        adjusted = [t - common_start for t in timestamps if common_start <= t <= end_time]\n",
    "        spike_counts, _ = np.histogram(adjusted, bins=bins)\n",
    "        rates = spike_counts / bin_size\n",
    "        adjusted_data[neuron] = adjusted\n",
    "        firing_rates[neuron] = rates\n",
    "\n",
    "    return adjusted_data, firing_rates\n",
    "\n",
    "def save_firing_rates_to_excel(firing_rates, filename='actualdata_firing_rates.xlsx'):\n",
    "    \"\"\"\n",
    "    Saves firing rates per neuron to an Excel file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    firing_rates : dict\n",
    "        Dictionary of neuron-wise firing rates (list of floats).\n",
    "    filename : str\n",
    "        Excel filename to save output.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(firing_rates, orient='index').transpose()\n",
    "    df.insert(0, 'Interval', range(len(df)))\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "def calculate_and_save_dot_products(firing_rates, window_size=3, filename='actualdata_dot_products_normalized.xlsx'):\n",
    "    \"\"\"\n",
    "    Calculates dot products between neuron firing rates using a sliding window approach,\n",
    "    min-max normalizes the upper triangle, and saves each window as a sheet in an Excel file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    firing_rates : dict\n",
    "        Dictionary of neuron firing rates (list of floats).\n",
    "    window_size : int\n",
    "        Number of bins per sliding window (default = 3).\n",
    "    filename : str\n",
    "        Excel file to save dot product matrices per window.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The upper triangle of each dot product matrix is min-max normalized per window.\n",
    "    This transformation preserves the rank ordering of dot product values while \n",
    "    mapping them to the [0, 1] interval, which improves interpretability and \n",
    "    prevents extreme firing rate magnitudes from dominating similarity estimates.\n",
    "    \n",
    "    Min-max normalization is preferred here because:\n",
    "    - It maintains the pairwise relationships between neurons within each time window\n",
    "    - It avoids assumptions about distributional shape (unlike z-scoring)\n",
    "    - It enables consistent thresholding or comparisons across animals or conditions\n",
    "    \"\"\"\n",
    "    neuron_names = list(firing_rates.keys())\n",
    "    num_neurons = len(neuron_names)\n",
    "    num_intervals = len(next(iter(firing_rates.values())))\n",
    "    num_windows = num_intervals - window_size + 1\n",
    "\n",
    "    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
    "        for start in range(num_windows):\n",
    "            matrix = np.zeros((num_neurons, num_neurons))\n",
    "            for i in range(num_neurons):\n",
    "                for j in range(i + 1, num_neurons):\n",
    "                    vec1 = firing_rates[neuron_names[i]][start:start + window_size]\n",
    "                    vec2 = firing_rates[neuron_names[j]][start:start + window_size]\n",
    "                    dot = np.dot(vec1, vec2)\n",
    "                    matrix[i, j] = dot\n",
    "\n",
    "            # Normalize upper triangle\n",
    "            upper_triangle = np.triu_indices(num_neurons, k=1)\n",
    "            scaler = MinMaxScaler()\n",
    "            norm_values = scaler.fit_transform(matrix[upper_triangle].reshape(-1, 1)).flatten()\n",
    "            matrix[upper_triangle] = norm_values\n",
    "\n",
    "            # Save matrix as DataFrame\n",
    "            df = pd.DataFrame(matrix, index=neuron_names, columns=neuron_names)\n",
    "            df.to_excel(writer, sheet_name=f'Window {start}')\n",
    "\n",
    "# === Example Usage ===\n",
    "\n",
    "# Assuming neuron_data is a dictionary: { 'neuron 1': [timestamp1, timestamp2, ...], ... }\n",
    "# For example:\n",
    "# neuron_data = {\n",
    "#     'neuron 1': [1654170515.718140, 1654170516.037640, ...],\n",
    "#     'neuron 2': [1654170520.123456, 1654170521.001234, ...]\n",
    "# }\n",
    "\n",
    "bin_size = 10\n",
    "duration = 600\n",
    "\n",
    "common_start_time = find_earliest_timestamp(neuron_data)\n",
    "adjusted_data, firing_rates = adjust_timestamps_and_calculate_rates(neuron_data, bin_size, common_start_time, total_duration=duration)\n",
    "\n",
    "save_firing_rates_to_excel(firing_rates, filename='actualdata_firing_rates.xlsx')\n",
    "calculate_and_save_dot_products(firing_rates, window_size=3, filename='actualdata_dot_products_normalized.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756871c-e18b-41be-bf02-93a0dc58dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the mean edge weight per neuron pair across all dot product matrices\n",
    "# stored in an Excel file (one sheet per time window), and saves results to Excel.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "def compute_mean_edge_weights(dot_product_excel_path, output_path='mean_edge_weights.xlsx'):\n",
    "    \"\"\"\n",
    "    Computes the mean edge weight for each neuron pair by averaging\n",
    "    dot products across all time windows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dot_product_excel_path : str\n",
    "        Path to the Excel file where each sheet represents a dot product matrix \n",
    "        for one time window. The matrix should have neurons as both row and column indices.\n",
    "\n",
    "    output_path : str, optional\n",
    "        Path to save the resulting mean edge weights as an Excel file. Default = 'mean_edge_weights.xlsx'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_edge_weights_df : pd.DataFrame\n",
    "        DataFrame with columns: ['Neuron Pair', 'Mean Edge Weight']\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function assumes that each sheet contains a symmetric matrix with neuron names \n",
    "    as both row and column labels. Only the upper triangle (excluding the diagonal) is used.\n",
    "    \"\"\"\n",
    "    xls = pd.ExcelFile(dot_product_excel_path)\n",
    "    edge_weights = {}\n",
    "\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, index_col=0)\n",
    "\n",
    "        for i, j in combinations(df.index, 2):  # Upper triangle only\n",
    "            if (i, j) not in edge_weights:\n",
    "                edge_weights[(i, j)] = []\n",
    "            edge_weights[(i, j)].append(df.at[i, j])\n",
    "\n",
    "    # Compute mean for each pair\n",
    "    mean_edge_weights = {\n",
    "        pair: np.mean(values) for pair, values in edge_weights.items()\n",
    "    }\n",
    "\n",
    "    mean_edge_weights_df = pd.DataFrame(\n",
    "        list(mean_edge_weights.items()),\n",
    "        columns=['Neuron Pair', 'Mean Edge Weight']\n",
    "    )\n",
    "\n",
    "    # Save to Excel\n",
    "    mean_edge_weights_df.to_excel(output_path, index=False)\n",
    "    return mean_edge_weights_df\n",
    "\n",
    "# === Run the function directly ===\n",
    "\n",
    "dot_product_file = \"actualdata_dot_products_normalized.xlsx\"   # Path to input file\n",
    "output_file = \"actualdata_mean_edge_weights.xlsx\"              # Path to output file\n",
    "\n",
    "mean_edge_weights_df = compute_mean_edge_weights(dot_product_file, output_file)\n",
    "print(\"First 5 rows of mean edge weights:\")\n",
    "print(mean_edge_weights_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d526e-02a3-4880-b4b0-b186b8270937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates ISIs, shuffles ISIs to generate null spike trains for all neurons,\n",
    "# plots ISI and spike trains for one example neuron, and saves the full set.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_isi(timestamps):\n",
    "    \"\"\"\n",
    "    Computes interspike intervals (ISI) from a list of spike timestamps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timestamps : list of float\n",
    "        Spike timestamps for a single neuron.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of ISIs.\n",
    "    \"\"\"\n",
    "    return np.diff(sorted(timestamps))\n",
    "\n",
    "def shuffle_isi_and_create_spike_trains(isi, num_shuffles=1000):\n",
    "    \"\"\"\n",
    "    Shuffles ISIs and creates null spike trains from cumulative sums.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    isi : array-like\n",
    "        Original ISIs to shuffle.\n",
    "    num_shuffles : int\n",
    "        Number of shuffled spike trains to generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of np.ndarray\n",
    "        List of shuffled spike trains.\n",
    "    \"\"\"\n",
    "    spike_trains = []\n",
    "    for _ in range(num_shuffles):\n",
    "        shuffled = np.random.permutation(isi)\n",
    "        spike_train = np.cumsum(shuffled)\n",
    "        spike_trains.append(spike_train)\n",
    "    return spike_trains\n",
    "\n",
    "def plot_isi_and_spike_trains(original_timestamps, shuffled_spike_trains, filename='isi_and_spike_train_plots.png'):\n",
    "    \"\"\"\n",
    "    Plots original and shuffled ISI distributions and spike trains.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_timestamps : list of float\n",
    "        Original spike timestamps.\n",
    "    shuffled_spike_trains : list of np.ndarray\n",
    "        Shuffled spike trains.\n",
    "    filename : str\n",
    "        Output image filename.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(15, 25))\n",
    "\n",
    "    # Plot original ISI\n",
    "    original_isi = calculate_isi(original_timestamps)\n",
    "    axs[0].hist(original_isi, bins=50, color='blue', alpha=0.7)\n",
    "    axs[0].set_title('Original ISI Distribution')\n",
    "    axs[0].set_xlabel('Interspike Interval (s)')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "\n",
    "    # Plot shuffled ISI\n",
    "    shuffled_isi = calculate_isi(shuffled_spike_trains[0])\n",
    "    axs[1].hist(shuffled_isi, bins=50, color='red', alpha=0.7)\n",
    "    axs[1].set_title('Shuffled ISI Distribution (1st Shuffle)')\n",
    "    axs[1].set_xlabel('Interspike Interval (s)')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "\n",
    "    # Raster plots\n",
    "    axs[2].eventplot(original_timestamps, color='blue')\n",
    "    axs[2].set_title('Original Spike Train')\n",
    "    axs[2].set_xlabel('Time (s)')\n",
    "\n",
    "    for i in range(2):\n",
    "        axs[3+i].eventplot(shuffled_spike_trains[i], color='red')\n",
    "        axs[3+i].set_title(f'Shuffled Spike Train {i+1}')\n",
    "        axs[3+i].set_xlabel('Time (s)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def save_shuffled_spike_trains(spike_trains_dict, filename='shuffled_spike_trains.npy'):\n",
    "    \"\"\"\n",
    "    Saves shuffled spike trains dictionary to disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spike_trains_dict : dict\n",
    "        Dictionary of neuron name → list of shuffled spike trains.\n",
    "    filename : str\n",
    "        Path to .npy file for saving.\n",
    "    \"\"\"\n",
    "    np.save(filename, spike_trains_dict)\n",
    "\n",
    "def load_shuffled_spike_trains(filename='shuffled_spike_trains.npy'):\n",
    "    \"\"\"\n",
    "    Loads shuffled spike trains from a saved .npy file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the .npy file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of neuron name → list of shuffled spike trains.\n",
    "    \"\"\"\n",
    "    return np.load(filename, allow_pickle=True).item()\n",
    "\n",
    "# === Example Usage for Full Dataset ===\n",
    "\n",
    "# Replace this with your full neuron dataset (10+ neurons)\n",
    "neuron_data = {\n",
    "    'neuron 1': [1654170515.7, 1654170516.0, 1654170517.0, 1654170519.0],\n",
    "    'neuron 2': [1654170515.8, 1654170516.4, 1654170518.2, 1654170520.1],\n",
    "    'neuron 3': [1654170515.9, 1654170516.2, 1654170516.5, 1654170516.9],\n",
    "    # Add as many neurons as needed\n",
    "}\n",
    "\n",
    "# Shuffle ISIs for all neurons\n",
    "shuffled_spike_trains = {\n",
    "    neuron: shuffle_isi_and_create_spike_trains(calculate_isi(timestamps), num_shuffles=1000)\n",
    "    for neuron, timestamps in neuron_data.items()\n",
    "}\n",
    "\n",
    "# Visualize one neuron (e.g., neuron 2) as example\n",
    "plot_isi_and_spike_trains(\n",
    "    original_timestamps=neuron_data['neuron 2'],\n",
    "    shuffled_spike_trains=shuffled_spike_trains['neuron 2'],\n",
    "    filename='isi_and_spike_train_plots.png'\n",
    ")\n",
    "\n",
    "# Save all shuffled trains to disk\n",
    "save_shuffled_spike_trains(shuffled_spike_trains, filename='shuffled_spike_trains.npy')\n",
    "\n",
    "# Verify saved structure\n",
    "loaded = load_shuffled_spike_trains('shuffled_spike_trains.npy')\n",
    "print(f\"Loaded neurons: {list(loaded.keys())}\")\n",
    "print(f\"Shuffles for neuron 2: {len(loaded['neuron 2'])}\")\n",
    "print(f\"First 5 spikes of first shuffle: {loaded['neuron 2'][0][:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f690b-23ac-4bd7-864c-b15d890de778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script evaluates the significance of functional connectivity edge weights\n",
    "# by comparing actual edge weights to a null distribution built from shuffled ISI spike trains.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import percentileofscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === STEP 1: Firing Rate Binning ===\n",
    "def calculate_firing_rates(timestamps, bin_size, total_duration):\n",
    "    \"\"\"\n",
    "    Converts spike timestamps into binned firing rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timestamps : list of float\n",
    "        Spike times for a neuron.\n",
    "    bin_size : int\n",
    "        Bin width in seconds.\n",
    "    total_duration : int\n",
    "        Total session duration in seconds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rates : np.ndarray\n",
    "        Firing rate per bin.\n",
    "    \"\"\"\n",
    "    bins = np.arange(0, total_duration + bin_size, bin_size)\n",
    "    spike_counts, _ = np.histogram(timestamps, bins=bins)\n",
    "    return spike_counts / bin_size\n",
    "\n",
    "# === STEP 2: Dot Product Over Sliding Windows ===\n",
    "def calculate_dot_products_from_rates(firing_rates, window_size=3):\n",
    "    \"\"\"\n",
    "    Computes dot product matrices for each sliding window of firing rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    firing_rates : dict\n",
    "        Neuron → firing rate vector.\n",
    "    window_size : int\n",
    "        Number of bins per dot product window.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of pd.DataFrame\n",
    "        List of dot product matrices (upper triangle normalized).\n",
    "    \"\"\"\n",
    "    neurons = sorted(firing_rates.keys(), key=lambda x: int(x.split(' ')[1]))\n",
    "    num_windows = len(firing_rates[neurons[0]]) - window_size + 1\n",
    "    dot_product_matrices = []\n",
    "\n",
    "    for start in range(num_windows):\n",
    "        matrix = pd.DataFrame(0.0, index=neurons, columns=neurons)\n",
    "        for i, n1 in enumerate(neurons):\n",
    "            for j, n2 in enumerate(neurons):\n",
    "                if i < j:\n",
    "                    vec1 = firing_rates[n1][start:start + window_size]\n",
    "                    vec2 = firing_rates[n2][start:start + window_size]\n",
    "                    dot = np.dot(vec1, vec2)\n",
    "                    matrix.at[n1, n2] = dot\n",
    "\n",
    "        # Normalize the upper triangle using MinMaxScaler\n",
    "        upper = np.triu_indices(len(neurons), k=1)\n",
    "        matrix.values[upper] = MinMaxScaler().fit_transform(matrix.values[upper].reshape(-1, 1)).flatten()\n",
    "        dot_product_matrices.append(matrix)\n",
    "\n",
    "    return dot_product_matrices\n",
    "\n",
    "# === STEP 3: Build Null Distribution ===\n",
    "def build_edge_weight_distribution(shuffled_spike_trains, bin_size=10, duration=600, window_size=3):\n",
    "    \"\"\"\n",
    "    Aggregates mean edge weights for all neuron pairs across shuffled spike trains.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shuffled_spike_trains : dict\n",
    "        Neuron → list of shuffled spike trains.\n",
    "    bin_size : int\n",
    "        Width of each bin in seconds.\n",
    "    duration : int\n",
    "        Total session length.\n",
    "    window_size : int\n",
    "        Sliding window size for dot products.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    edge_weight_distribution : list of float\n",
    "        List of mean edge weights across all shuffled networks.\n",
    "    \"\"\"\n",
    "    edge_weight_distribution = []\n",
    "    neurons = sorted(shuffled_spike_trains.keys(), key=lambda x: int(x.split(' ')[1]))\n",
    "\n",
    "    for shuffle_idx in range(len(shuffled_spike_trains[neurons[0]])):\n",
    "        # Compute firing rates for this shuffle\n",
    "        rates = {\n",
    "            neuron: calculate_firing_rates(shuffled_spike_trains[neuron][shuffle_idx], bin_size, duration)\n",
    "            for neuron in neurons\n",
    "        }\n",
    "\n",
    "        # Compute sliding-window dot products\n",
    "        dp_matrices = calculate_dot_products_from_rates(rates, window_size)\n",
    "\n",
    "        # Compute mean edge weights across all windows\n",
    "        mean_weights = {}\n",
    "        for matrix in dp_matrices:\n",
    "            for i in range(len(neurons)):\n",
    "                for j in range(i + 1, len(neurons)):\n",
    "                    pair = (neurons[i], neurons[j])\n",
    "                    mean_weights.setdefault(pair, []).append(matrix.iat[i, j])\n",
    "\n",
    "        # Average the mean edge weights for each neuron pair and add to null distribution\n",
    "        for pair, values in mean_weights.items():\n",
    "            edge_weight_distribution.append(np.mean(values))\n",
    "\n",
    "        # Optionally save dot product matrices from first 3 shuffles\n",
    "        if shuffle_idx < 3:\n",
    "            with pd.ExcelWriter(f'shuffled_dot_products_{shuffle_idx}.xlsx') as writer:\n",
    "                for idx, mat in enumerate(dp_matrices):\n",
    "                    mat.to_excel(writer, sheet_name=f'Window {idx}')\n",
    "\n",
    "    return edge_weight_distribution\n",
    "\n",
    "# === STEP 4: Significance Testing ===\n",
    "def test_edge_significance(actual_edge_path, null_distribution, output_path='significance_results_with_p_values.xlsx'):\n",
    "    \"\"\"\n",
    "    Compares actual edge weights to a null distribution and computes significance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual_edge_path : str\n",
    "        Excel file with real edge weights (columns: 'Neuron Pair', 'Mean Edge Weight').\n",
    "    null_distribution : list of float\n",
    "        List of edge weights from null model.\n",
    "    output_path : str\n",
    "        Excel path to save results.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame with edge significance and p-values.\n",
    "    \"\"\"\n",
    "    actual_df = pd.read_excel(actual_edge_path)\n",
    "    actual_edges = dict(actual_df[['Neuron Pair', 'Mean Edge Weight']].values)\n",
    "\n",
    "    significance = {}\n",
    "    p_values = {}\n",
    "\n",
    "    for pair, actual in actual_edges.items():\n",
    "        percentile = percentileofscore(null_distribution, actual)\n",
    "        p = min(percentile, 100 - percentile) / 100\n",
    "        p_values[pair] = p\n",
    "        significance[pair] = 'TRUE' if p < 0.05 else 'FALSE'\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'Neuron Pair': list(significance.keys()),\n",
    "        'P-Value': list(p_values.values()),\n",
    "        'Significance': list(significance.values())\n",
    "    })\n",
    "\n",
    "    def format_p(p):\n",
    "        if p < 0.001:\n",
    "            return f'***{p:.4f}'\n",
    "        elif p < 0.01:\n",
    "            return f'**{p:.4f}'\n",
    "        elif p < 0.05:\n",
    "            return f'*{p:.4f}'\n",
    "        else:\n",
    "            return f'{p:.4f}'\n",
    "\n",
    "    results['P-Value with Asterisks'] = results['P-Value'].apply(format_p)\n",
    "    results.to_excel(output_path, index=False)\n",
    "    return results\n",
    "\n",
    "# === STEP 5: Null Distribution Plot ===\n",
    "def plot_edge_weight_distribution(null_distribution, bins=50):\n",
    "    \"\"\"\n",
    "    Plots histogram of null edge weight distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    null_distribution : list of float\n",
    "        Edge weights from shuffled spike trains.\n",
    "    bins : int\n",
    "        Number of histogram bins.\n",
    "    \"\"\"\n",
    "    plt.hist(null_distribution, bins=bins, alpha=0.75)\n",
    "    plt.title('Edge Weight Null Distribution')\n",
    "    plt.xlabel('Mean Edge Weight')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# === FULL PIPELINE RUN ===\n",
    "\n",
    "# Load shuffled spike trains\n",
    "shuffled_spike_trains = np.load(\"shuffled_spike_trains.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Build null distribution of edge weights from all shuffles\n",
    "null_dist = build_edge_weight_distribution(\n",
    "    shuffled_spike_trains,\n",
    "    bin_size=10,\n",
    "    duration=600,\n",
    "    window_size=3\n",
    ")\n",
    "\n",
    "# Plot null distribution\n",
    "plot_edge_weight_distribution(null_dist)\n",
    "\n",
    "# Test real edge weights against null\n",
    "results_df = test_edge_significance(\n",
    "    actual_edge_path='actualdata_mean_edge_weights.xlsx',\n",
    "    null_distribution=null_dist,\n",
    "    output_path='significance_results_with_p_values.xlsx'\n",
    ")\n",
    "\n",
    "# Display first few significant edges\n",
    "print(\"Significance testing results (top 5 rows):\")\n",
    "print(results_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch22)",
   "language": "python",
   "name": "pytorch22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
